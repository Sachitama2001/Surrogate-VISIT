0. 想定しているデータ構造の整理

context_x : (B, T_ctx, D_dyn)
過去の動的入力（コンテキスト部分）

static_x : (B, D_static)
サイトなどの静的特徴

future_known : (B, T_pred, D_dyn)
予測期間中に既知の動的入力（将来の気象など）

target_y : (B, T_pred, D_out)
予測ターゲット（GPP, NPP, NEP, Rh の4次元想定）

モデル構造（現状の大枠）：

static_mlp(static_x) → (h0, c0) を生成

static_embedding(static_x) → 各時刻に expand → context_x / future_known と concat

LSTM で時系列処理

output_head で (hidden → 出力)

将来ステップはループで 1 ステップずつ予測

1. 正規化（Normalization）まわりの改善案
1-1. LSTM 内部に対して BatchNorm を使わない 方針の確認

通常の nn.LSTM のゲート計算や隠れ状態に対して、標準的な BatchNorm を直接入れるのは非推奨 という話をしました。

理由：時刻ごとにバッチ統計が変わり、再帰構造を壊しやすい。

→ 現状どおり、LSTM の内部には BatchNorm を入れない設計は妥当 という整理。

1-2. 入力側に BatchNorm をかける代替案（任意）

時系列入力（context_x, future_known）＋静的埋め込みを結合した
context_input : (B, T_ctx, D_dyn + D_emb) に対して、

(B*T_ctx, D_in) に reshape して nn.BatchNorm1d(D_in) を適用する案はあり得る、という話をしました。

ただし、

シーケンス長が長く、バッチが小さいと統計がノイジーになる

実務上は データ前処理での標準化（mean/std 正規化） の方が扱いやすい
という注意も共有済み。

1-3. LSTM 内部に正規化を入れるなら LayerNorm 推奨

もし再帰内部で正規化したい場合は

nn.LSTM(..., layer_norm=True)（PyTorch 2.x 系）

あるいはカスタム LSTM + nn.LayerNorm(hidden_size)

といった Layer Normalization ベース の方が一貫して安定という指摘をしました。

2. 静的パラメータの統合方法に関する設計案
2-1. 現状方式の評価：静的埋め込みを各時刻に複製して concat

現状の設計：

static_embedding(static_x) → (B, D_emb)

unsqueeze(1) → (B, 1, D_emb)

.expand(-1, T, -1) → (B, T, D_emb)

torch.cat([dynamic_x, static_emb_expanded], dim=-1) → (B, T, D_dyn + D_emb)

これは時系列モデル（LSTM/Transformer）における 非常に一般的で妥当な方法 である、と評価済み。

ただし「静的情報がただの追加特徴としてしか扱われない」という観点から、
タスクによってはさらにリッチな統合方法に改善余地あり、という話をしました。

2-2. FiLM 的な 動的入力の変調（modulation） 案

静的特徴を使って、動的入力 x_t をスケール＆バイアスする案：

例：

gamma, beta = f(static_x)   # MLP 等で生成
x_t_mod = gamma * x_t + beta


その後で LSTM に投入：

next_input = concat(x_t_mod, static_emb)  # または x_t_mod のみ


効果：

静的パラメータが直接、動的入力の「意味」「スケール」「オフセット」を変調する。

土地特性が気象変数の影響度を変えるような問題設定に特に有効な可能性。

2-3. 静的特徴を LSTM のゲートに直接注入 する案

LSTM の各ゲート計算に static_emb を add する拡張 LSTM：

例：

i_t = σ( W_i x_t + U_i h_{t-1} + V_i static_emb )
f_t = σ( W_f x_t + U_f h_{t-1} + V_f static_emb )
o_t = σ( W_o x_t + U_o h_{t-1} + V_o static_emb )
g_t = tanh( W_g x_t + U_g h_{t-1} + V_g static_emb )


静的情報が 記憶の書き込み・保持・読み出し に直接影響する構造になるため、表現力が高い。

※ これについては「実装はやや複雑になるが、構造として自然」というコメントまで。

3. Attention による静的情報の利用方法（3パターン）

「静的パラメータに対する attention を LSTM の各時刻で利用する」という方向で、
以下の 3 タイプを整理しました（命名は会話中のものに合わせています）。

3-1. Type-A：静的特徴集合への attention → 文脈ベクトルを LSTM 入力へ concat

イメージ：

静的特徴を「S 個のスロットを持つ埋め込み集合」として持つ：

static_features : (B, S, D_s)

各時刻の LSTM hidden h_t : (B, D_h) を query として attention：

attention_t      = softmax( q(h_t) ⋅ k(static_features)^T )
static_context_t = Σ attention_t * v(static_features)
x_t' = concat( x_t, static_context_t )


特徴：

「どの静的特徴を参照するか」が 時刻ごとに変化 し得る。

実装コストが比較的低く、元の concat 型と近い構造。

3-2. Type-B：attention で得た静的コンテキストで動的入力を変調

static_context_t を直接 x_t に作用させる案：

例：

static_context_t = Attention(h_t, static_features)

x_t_mod = x_t + static_context_t
# or
x_t_mod = gamma(static_context_t) * x_t + beta(static_context_t)


特徴：

入力次元をむやみに膨らませずに済む（concat しなくてよい構造も取り得る）。

「静的 × 動的」の相互作用を、FiLM に近い形で時間ごとに変えられる。

3-3. Type-C：attention で得た静的コンテキストを LSTM ゲートに注入

static_context_t = Attention(h_{t-1}, static_features) を計算し、

ゲートに加算：

i_t = σ( W_i x_t + U_i h_{t-1} + V_i static_context_t )
...


特徴：

静的情報が、LSTM の内部状態更新ルールに直接入り込む構造。

最も表現力は高いが、LSTM 実装をカスタム化する必要がある。

4. デコーダ（将来ステップ）の「オートレグレッシブ性」に関する指摘
4-1. 現状の問題点（挙動）

将来ステップのループ内で、

last_output = self.output_head(context_output[:, -1, :])  # 初期値

for t in range(prediction_len):
    # teacher forcing で last_output を target_y で上書きする部分はあるが…
    known_input = future_known[:, t, :]  # (B, D_dyn)
    next_input = known_input             # last_output は concat されていない
    ...
    lstm_out, (h, c) = self.lstm(next_input_with_static, (h, c))
    next_output = self.output_head(lstm_out[:, 0, :])
    predictions.append(next_output)
    last_output = next_output


という形になっていて、

last_output は LSTM の入力には使われていない

teacher_forcing_ratio は last_output を真の target_y に差し替えるだけで、
実際の LSTM 入力には反映されていない

→ 結果として、
デコーダは実質「future_known だけを入力にした非オートレグレッシブ LSTM」になっている
（出力のフィードバックは起きていない）という指摘をしました。

4-2. 真のオートレグレッシブにする改善案（方向性のみ）

もし「出力も次ステップの入力に使う」設計にしたいなら、例えば：

next_input = torch.cat([known_input, last_output], dim=-1)  # (B, D_dyn + D_out)


を LSTM 入力側に含め、
teacher forcing では last_output を target_y[:, t-1, :] に置き換えることで
出力の自己回帰 を実現できる、という話をしました。

※詳細なコードはまだ書いていませんが、「現状は teacher forcing が有効に機能していない」という点まで指摘済みです。

5. バッチと BatchNorm の挙動に関する整理（仕様理解）

これは「改善案」というより 仕様上の理解 ですが、
関連するためまとめておきます。

通常の線形層・LSTM は、バッチ次元は完全に独立なサンプルの集合として扱い、
サンプル同士は推論上相互作用しない。

BatchNorm を使う場合のみ、

訓練時はバッチ全体の平均・分散を用いて正規化

推論時は running mean/var を用いた固定変換

話の流れとして：

「静的 MLP 等に BatchNorm を入れるのは妥当」

「LSTM 内部（再帰部分）には BatchNorm を入れないのが望ましい」
という整理をしています。